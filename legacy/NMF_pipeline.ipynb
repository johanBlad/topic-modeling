{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHANGE DIR TO ROOT\n",
      "Already downloaded a model for the 'sv' language\n"
     ]
    }
   ],
   "source": [
    "# Necessary for importing modules from a sub-directory\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "if os.getcwd().split('/')[-1] == 'notebooks':\n",
    "    print(\"CHANGE DIR TO ROOT\")\n",
    "    os.chdir(r\"../\")\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "from preprocess import article_text_to_tfidf, build_ne_gold_phraser, get_gold_ngrams, apply_ngrams\n",
    "from evaluation import aggregate_metrics, get_metric_umass, get_metric_cv\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim import corpora\n",
    "\n",
    "from efselabwrapper import run_annotation_pipeline, run_processing_pipeline, run_processing_pipeline_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMBER OF ARTICLES IN CORPUS: 10000\n"
     ]
    }
   ],
   "source": [
    "if 1==1:\n",
    "    DATASET_TYPE = 'DN'\n",
    "    DATASET_SIZE = '10000'\n",
    "    DATASET_INDEX = '1'\n",
    "    data_dir = f'var/data_prod_noun_propn'\n",
    "    corpus = pickle.load(open(f'{data_dir}/efselab_{DATASET_TYPE}_{DATASET_SIZE}_{DATASET_INDEX}.pkl', 'rb'))\n",
    "\n",
    "else:\n",
    "    raw_df = pickle.load(open(f'var/data/DN_10000_1.pkl', 'rb'))\n",
    "    raw_strings = [row[1].content_text for row in raw_df.iterrows()]\n",
    "\n",
    "    pos_tags = ['NOUN']\n",
    "    corpus = run_processing_pipeline(raw_strings, pos_tags)\n",
    "    \n",
    "ext_corpus = pickle.load(open(f'{data_dir}/efselab_extrinsic_20000.pkl', 'rb'))\n",
    "NUM_ARTICLES = len(corpus)\n",
    "print(f'NUMBER OF ARTICLES IN CORPUS: {NUM_ARTICLES}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "c1 = pickle.load(open(f'{data_dir}/efselab_{DATASET_TYPE}_{DATASET_SIZE}_1.pkl', 'rb'))\n",
    "c2 = pickle.load(open(f'{data_dir}/efselab_{DATASET_TYPE}_{DATASET_SIZE}_2.pkl', 'rb'))\n",
    "c3 = pickle.load(open(f'{data_dir}/efselab_{DATASET_TYPE}_{DATASET_SIZE}_3.pkl', 'rb'))\n",
    "c4 = pickle.load(open(f'{data_dir}/efselab_{DATASET_TYPE}_{DATASET_SIZE}_4.pkl', 'rb'))\n",
    "c5 = pickle.load(open(f'{data_dir}/efselab_{DATASET_TYPE}_{DATASET_SIZE}_5.pkl', 'rb'))\n",
    "corpus = c1 + c2 + c3 + c4 + c5\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat = [e for sublist in corpus for e in sublist]\n",
    "x = [e for e in flat if 'åring' == e]\n",
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tpe = 'DN' # DN, BN, DN+BN+HD\n",
    "#corpus = pickle.load(open(f'var/data_survey/efselab_parsed_{tpe}_10000_survey.pkl', 'rb'))\n",
    "#print(len(corpus))\n",
    "\n",
    "#a1 = corpus[10000]; a2 = corpus[10001]; a3 = corpus[10002] \n",
    "#tp = [e for e in corpus[:10000] if e == a1 or e == a3 or e == a2]\n",
    "#tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pickle.load(open(f'var/data/preprocessed_NOUN_prod_4_DEC.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSIFICATION = False\n",
    "if CLASSIFICATION:\n",
    "    test_data = pickle.load(open(f'var/data/efselab_parsed_{DATASET_TYPE}_4000_1.pkl', 'rb'))\n",
    "    test_data2 = pickle.load(open(f'var/data/efselab_parsed_{DATASET_TYPE}_4000_2.pkl', 'rb'))\n",
    "    test_data3 = pickle.load(open(f'var/data/efselab_parsed_{DATASET_TYPE}_4000_3.pkl', 'rb'))\n",
    "    test_data4 = pickle.load(open(f'var/data/efselab_parsed_{DATASET_TYPE}_4000_4.pkl', 'rb'))\n",
    "    test_data5 = pickle.load(open(f'var/data/efselab_parsed_{DATASET_TYPE}_4000_5.pkl', 'rb'))\n",
    "    test_truncated = test_data + test_data2 + test_data3 + test_data4 + test_data5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKL_NMF_frobenius_mu\n"
     ]
    }
   ],
   "source": [
    "# Preprocess varibles\n",
    "MAX_DF = 0.95 # float (0.95)\n",
    "MIN_DF = 2    # int: 2\n",
    "NGRAM = False # False | True\n",
    "if NGRAM is True:\n",
    "    NGRAM_OPT = 'gold+ne' # 'ext+int' | 'int' \n",
    "else:\n",
    "    NGRAM_OPT = '-'\n",
    "\n",
    "# Model variables\n",
    "\n",
    "#NUM_ARTICLES = len(df.index)\n",
    "NUM_ARTICLES = len(corpus)\n",
    "NUM_TOPICS = 40\n",
    "\n",
    "NMF_NORM = 'frobenius' # ‘frobenius’ | ‘kullback-leibler’ | ‘itakura-saito’\n",
    "NMF_SOLVER = 'mu' # ‘cd’ | ‘mu’\n",
    "MAX_ITERATIONS = 500\n",
    "INIT = None # None | ‘random’ | ‘nndsvd’ | ‘nndsvda’ | ‘nndsvdar’ | ‘custom’\n",
    "ALPHA = 0 # float\n",
    "L1_RATIO = 0.0 # float\n",
    "\n",
    "MODEL_NAME = f'SKL_NMF_{NMF_NORM}_{NMF_SOLVER}'\n",
    "if INIT is not None:\n",
    "    MODEL_NAME = f'{MODEL_NAME}_init-{str(INIT)}'\n",
    "if (ALPHA != 0):\n",
    "    MODEL_NAME = f'{MODEL_NAME}_alpha-{ALPHA}_L1-{L1_RATIO}'\n",
    "print(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (NGRAM is True):\n",
    "    # Load NE_grams and gold_grams\n",
    "    NE_GRAMS_FILE = f'var/ne_ngrams_{DATASET_TYPE}_{DATASET_SIZE}_{DATASET_INDEX}.pkl'\n",
    "    ne_ngrams = pickle.load(open(NE_GRAMS_FILE, 'rb'))\n",
    "    gold_ngrams = get_gold_ngrams()\n",
    "    empty_grams = {\n",
    "        'bigrams': [],\n",
    "        'trigrams': []\n",
    "    }\n",
    "\n",
    "    if NGRAM_OPT == 'gold+ne':\n",
    "        phraser = build_ne_gold_phraser(ne_ngrams, gold_ngrams)\n",
    "    else:\n",
    "        phraser = build_ne_gold_phraser(empty_grams, gold_ngrams)\n",
    "        \n",
    "    corpus = apply_ngrams(corpus, phraser)\n",
    "    ext_corpus = apply_ngrams(ext_corpus, phraser)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Features from tfidf is 128401\n"
     ]
    }
   ],
   "source": [
    "# Create TF-IDF\n",
    "vectorizer = TfidfVectorizer(\n",
    "    lowercase=False,\n",
    "    tokenizer=lambda x: x,\n",
    "    max_df=MAX_DF, \n",
    "    min_df=MIN_DF, max_features=None, use_idf=True)\n",
    "tf_idf = vectorizer.fit_transform(corpus)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "NUM_FEATURES = len(feature_names)\n",
    "print(f'#Features from tfidf is {NUM_FEATURES}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "t_start = time.time()\n",
    "\n",
    "model = NMF(random_state=0, tol=0.0001, verbose=0, shuffle=False,\\\n",
    "            n_components=NUM_TOPICS, beta_loss=NMF_NORM, solver=NMF_SOLVER, \\\n",
    "            init=INIT, alpha=ALPHA, l1_ratio=L1_RATIO, max_iter=MAX_ITERATIONS)\n",
    "\n",
    "W = model.fit_transform(tf_idf, y=feature_names)\n",
    "H = model.components_\n",
    "ITERATIONS = model.n_iter_\n",
    "\n",
    "RUN_TIME = round(time.time() - t_start, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLASSIFICATION:\n",
    "    # Apply model as classifier for unseen data\n",
    "    test_tf_idf = vectorizer.transform(test_truncated)\n",
    "    W = model.transform(test_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of topics (nested list)\n",
    "def get_NMF_topics(model, feature_names, n_top_words):\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topics.append([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "    return topics\n",
    "\n",
    "N_TOP_WORDS = 10\n",
    "topics = get_NMF_topics(model, feature_names, N_TOP_WORDS)\n",
    "#topics_full = get_NMF_topics(model, feature_names, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tvar = [[e for e in t if e not in [w for sl in topics_full[:i]+topics_full[i+1:] for w in sl]] for i,t in enumerate(topics_full)]\n",
    "#print(tvar)\n",
    "#topics_tf_idf = vectorizer.transform(tvar+[])\n",
    "#W_test = model.transform(topics_tf_idf) \n",
    "\n",
    "#article_topics = {}\n",
    "#for i in range(len(W_test)):\n",
    "#    tt = sorted([(f'T{k}', f'{num:.5f}') for k,num in enumerate(W_test[i])], key=lambda x: x[1], reverse=True)[:5]\n",
    "#    article_topics[i] = tt\n",
    "#    print(f'Doc{i}: ', tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#topics = []\n",
    "#for topic_idx, topic in enumerate(model.components_):\n",
    "#    topics.append([feature_names[i] for i in np.flip(topic.argsort()[-11:])])\n",
    "#df_topic_log = pd.DataFrame(topics)\n",
    "#tsave = f'metric_log/topics/df_topics_{DATASET_TYPE}{DATASET_SIZE}_{DATASET_INDEX}_{WC}'\n",
    "#df_topic_log.to_csv(tsave+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format Gensim components for Coherence metrics\n",
    "DO_EVAL = False\n",
    "if DO_EVAL:\n",
    "    ext_gensim_dict = corpora.Dictionary(ext_corpus)\n",
    "\n",
    "    tok2id = ext_gensim_dict.token2id\n",
    "    flat_topic_terms = [term for topic in topics for term in topic]\n",
    "    topic_terms_not_in_ext_corpus = [[term] for term in flat_topic_terms if term not in tok2id]\n",
    "\n",
    "    smooth_corpus = ext_corpus + topic_terms_not_in_ext_corpus\n",
    "    smooth_gensim_dict = corpora.Dictionary(smooth_corpus)\n",
    "    print(f'{len(topic_terms_not_in_ext_corpus)} number of terms not in extrinsic corpus added as separate one-term documents')\n",
    "\n",
    "    int_gensim_dict = corpora.Dictionary(corpus)\n",
    "    int_gensim_corpus = [int_gensim_dict.doc2bow(text) for text in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Coherence metrics\n",
    "if DO_EVAL:\n",
    "    m_cv = get_metric_cv(topics, smooth_corpus, smooth_gensim_dict)\n",
    "    coh_cv = [e[0] for e in m_cv]\n",
    "    std_cv = [e[1] for e in m_cv]\n",
    "    m_umass = get_metric_umass(topics, int_gensim_dict, int_gensim_corpus)\n",
    "    coh_umass = [e[0] for e in m_umass]\n",
    "    std_umass = [e[1] for e in m_umass]\n",
    "\n",
    "    reduced_cv = aggregate_metrics(coh_cv)\n",
    "    reduced_cv['std'] = round(np.mean(std_cv), 4)\n",
    "    reduced_umass = aggregate_metrics(coh_umass)\n",
    "    reduced_umass['std'] = round(np.mean(std_umass), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate article assignment distinction\n",
    "#article_assignment_metric = [sum(sorted([num for num in W[i]], reverse=True)[:3]) for i in range(len(corpus))]\n",
    "#reduced_a = aggregate_metrics(article_assignment_metric)\n",
    "#print(reduced_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'avg': 0.5007, 'med': 0.4694, 'top': 0.872, 'bot': 0.2429}\n"
     ]
    }
   ],
   "source": [
    "# Calculate document-topic relative sparseness\n",
    "def doc_top_relative_sparseness(W, top_n):\n",
    "    res = []\n",
    "    for i in range(len(W[:])):\n",
    "        #topics = sorted([num for num in W[i]], reverse=True)    \n",
    "        topics = sorted(W[i], reverse=True)    \n",
    "        res.append(sum(topics[:top_n])/sum(topics))\n",
    "    return res\n",
    "\n",
    "top_n_topics = 1\n",
    "\n",
    "#relative_sparseness = doc_top_relative_sparseness(W, top_n_topics)\n",
    "relative_sparseness = [sum(sorted(W[i], reverse=True)[:top_n_topics])/(sum(W[i]) if sum(W[i]) > 0 else 1) for i in range(len(W[:]))]\n",
    "reduced_rs = aggregate_metrics(relative_sparseness)\n",
    "print(reduced_rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL STATISTICS\n",
      "\n",
      "SKL_NMF_frobenius_mu\n",
      "# Articles:  110034\n",
      "# Topics: 40\n",
      "NGRAM = False\n",
      "Iterations: 30\n",
      "Runtime:  72.079 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('MODEL STATISTICS\\n')\n",
    "print(MODEL_NAME)\n",
    "print('# Articles: ', NUM_ARTICLES)\n",
    "print('# Topics:', NUM_TOPICS)\n",
    "print('NGRAM = True') if NGRAM == 'True' else print('NGRAM = False')\n",
    "print('Iterations:', ITERATIONS)\n",
    "print('Runtime: ', RUN_TIME, 'seconds')\n",
    "print()\n",
    "\n",
    "if DO_EVAL:\n",
    "    print('C_v:\\t ', reduced_cv)\n",
    "    print('U_mass:\\t ', reduced_umass)\n",
    "    print(f'RS ({top_n_topics}):\\t ', reduced_rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: fråga jobb arbete verksamhet peng land samhälle kris möjlighet problem \n",
      "\n",
      "2: hus kvadratmeter pris krona ägare boyta säljare ägarbyte adress fastighet \n",
      "\n",
      "3: match poäng lag seger halvlek mål tränare säsong förlust spel \n",
      "\n",
      "4: polis presstalesperson plats händelse brott natt larm gärningsman vittne skada \n",
      "\n",
      "5: miljon krona kvartal resultat rörelseresultat omsättning vinst skatt nettoomsättning aktie \n",
      "\n",
      "6: patient vård läkare sjukvård sjuksköterska personal region vårdcentral hälsa behandling \n",
      "\n",
      "7: olycka räddningstjänst väg bil ambulans trafik personbil larm trafikolycka plats \n",
      "\n",
      "8: president land protest val premiärminister demonstrant demokrat ledare uttalande presidentval \n",
      "\n",
      "9: barn förälder familj förskola mamma skola pappa flicka liv pojke \n",
      "\n",
      "10: kommun boende kommunstyrelse äldreboende verksamhet ordförande äldreomsorg hemtjänst förskola område \n",
      "\n",
      "11: säsong klubb spelare kontrakt tränare sportchef liga division trupp spel \n",
      "\n",
      "12: aktie bolag stockholmsbörs börs kurs riktkurs finansinspektion innehav insynsregister köp \n",
      "\n",
      "13: kvinna våldtäkt våld misshandel jämställdhet lägenhet brott övergrepp ofredande förhör \n",
      "\n",
      "14: brand räddningstjänst larm byggnad plats lägenhet natt mordbrand eld rök \n",
      "\n",
      "15: liv stad utställning familj museum bild tal konstnär verk vän \n",
      "\n",
      "16: kvartal miljard euro rapport prognos försäljning analytiker mkr period tillväxt \n",
      "\n",
      "17: parti liberal val politik partiledare väljare socialdemokrat moderat riksdag fråga \n",
      "\n",
      "18: skola elev lärare rektor undervisning gymnasium klass grundskola skolinspektion gymnasieskola \n",
      "\n",
      "19: bostad vd lägenhet projekt pressmeddelande styrelse hyresrätt roll område bostadsrätt \n",
      "\n",
      "20: byrå kund kampanj varumärke resumé kommunikation vd marknadschef marknad director \n",
      "\n",
      "21: region län folkhälsomyndighet antal smittspridning smittskyddsläkare sjukvård pressmeddelande hälsa coronavirus \n",
      "\n",
      "22: mord åklagare tingsrätt brott fängelse utredning åtal hovrätt rättegång advokat \n",
      "\n",
      "23: bil förare elbil fordon körning natt bilbrand parkering rattfylleri tesla \n",
      "\n",
      "24: fastighet kvadratmeter affär hyresgäst förvärv lokal yta byggnad redaktion hyresavtal \n",
      "\n",
      "25: bolag vd verksamhet styrelse anställd företag ägare investerare investering konkurs \n",
      "\n",
      "26: mål seger boll poäng halvlek säsong period hemmalag ledning målvakt \n",
      "\n",
      "27: film regissör skådespelare premiär kampanj filmfestival reklamfilm huvudroll roll manus \n",
      "\n",
      "28: media tidning chefredaktör medium journalistik annonsör läsare innehåll journalist metro \n",
      "\n",
      "29: tävling final meter lopp seger helg lördag söndag häst plats \n",
      "\n",
      "30: företag produkt varumärke anställd konsument näringsliv företagare hållbarhet konkurs användare \n",
      "\n",
      "31: lag spelare tränare trupp division grupp förbundskapten spel turnering landslag \n",
      "\n",
      "32: bok författare roman förlag berättelse litteratur liv historia läsare bibliotek \n",
      "\n",
      "33: bank penningtvätt kund storbank ränta vd styrelse krona granskning finansinspektion \n",
      "\n",
      "34: regering förslag utsläpp utredning riksdag myndighet stöd åtgärd uppdrag minister \n",
      "\n",
      "35: dollar miljon fat oljepris intäkt analytiker amazon vinst förväntning tesla \n",
      "\n",
      "36: studie forskare resultat läkemedel behandling vaccin virus sjukdom risk universitet \n",
      "\n",
      "37: låt musik artist album band konsert scen publik skiva melodifestival \n",
      "\n",
      "38: butik kund försäljning handel e_handel lokal restaurang produkt personal konkurs \n",
      "\n",
      "39: sjukhus ambulans mordförsök skada misshandel universitetssjukhus händelse operation bråk skadeläge \n",
      "\n",
      "40: börs index avtal ränta nedgång handel ekonomi uppgång centralbank marknad \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print topics with N top words\n",
    "print_topics = True\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        #message = \"Topic #%d: \" % topic_idx\n",
    "        message = str(topic_idx+1) +': '\n",
    "        message += \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        #message = \" \".join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message, '\\n')\n",
    "\n",
    "if print_topics:\n",
    "    print_top_words(model, feature_names, 10)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-0b89a55a8ad7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m73\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print(topics[73])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholded_articles = []\n",
    "threshold = np.max(W)*0.1\n",
    "for i in range(len(W)):\n",
    "    topics_for_doc = sorted([(f'T{k}', num) for k,num in enumerate(W[i])], key=lambda x: x[1], reverse=True)[:5]\n",
    "    if topics_for_doc[0][1] > threshold:\n",
    "        entry = (f'Doc{i}', [(e[0], f'{e[1]:.5f}') for e in topics_for_doc])\n",
    "        thresholded_articles.append(entry)\n",
    "print(len(thresholded_articles))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholded_articles[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "article_topics = {}\n",
    "max_w, w_i = 0, 0\n",
    "for i in range(len(W)):\n",
    "    topics_for_doc = sorted([(f'T{k}', f'{num:.5f}') for k,num in enumerate(W[i])], key=lambda x: x[1], reverse=True)[:5]\n",
    "    article_topics[i] = topics_for_doc\n",
    "    if float(topics_for_doc[0][1]) > max_w:\n",
    "        max_w = float(topics_for_doc[0][1])\n",
    "        w_i = i\n",
    "    if i <= 100: \n",
    "        print(f'Doc{i}: ', topics_for_doc)\n",
    "print(max_w, w_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#article_topics = {}\n",
    "for i in range(len(W)):\n",
    "    topics_for_doc = sorted([(k, num) for k,num in enumerate(W[i])], key=lambda x: x[1], reverse=True)\n",
    "    if i >= 10000:\n",
    "        s = sum([e[1] for e in topics_for_doc])\n",
    "        m1 = f'({\" \".join(topics[topics_for_doc[0][0]])})*{round(topics_for_doc[0][1]/s, 3)}'\n",
    "        m2 = f'({\" \".join(topics[topics_for_doc[1][0]])})*{round(topics_for_doc[1][1]/s, 3)}'\n",
    "        m3 = f'({\" \".join(topics[topics_for_doc[2][0]])})*{round(topics_for_doc[2][1]/s, 3)}'\n",
    "        print(m1 + ', ' + m2 + ', ' + m3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to csv\n",
    "if False:\n",
    "    METRIC_FILENAME = f'metric_log/nmf_metrics_data={DATASET_TYPE}{NUM_ARTICLES}_single_2.csv'\n",
    "    DATE = datetime.now().strftime('%m%d-%H:%M')\n",
    "\n",
    "    fields=[DATE, MODEL_NAME, NUM_ARTICLES, NUM_TOPICS, ITERATIONS, RUN_TIME, NGRAM_OPT, NUM_FEATURES, \\\n",
    "            reduced_cv.get('avg'), reduced_cv.get('top'), reduced_cv.get('bot'), \\\n",
    "            reduced_umass.get('avg'), reduced_umass.get('top'), reduced_umass.get('bot'), \\\n",
    "            reduced_rs.get('avg'), reduced_rs.get('top'), reduced_rs.get('bot'), '_']\n",
    "\n",
    "    if os.path.exists(METRIC_FILENAME):  \n",
    "        with open(METRIC_FILENAME, 'a+', newline='\\n', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f, delimiter=',')\n",
    "            writer.writerow(fields)\n",
    "        f.close()\n",
    "    else:\n",
    "        with open(METRIC_FILENAME, 'w', newline='\\n', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f, delimiter=',')\n",
    "            headers=['date', 'model', '#articles', '#topics', 'iterations', 'time', 'ngram', '#features', \\\n",
    "            'cv_avg', 'cv_top', 'cv_bot', \\\n",
    "            'umass_avg', 'umass_top', 'umass_bot', \\\n",
    "            'rs_avg', 'rs_top', 'rs_bot', \\\n",
    "             '_']\n",
    "            writer.writerow(headers)\n",
    "            writer.writerow(fields)\n",
    "        f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
