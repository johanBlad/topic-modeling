{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHANGE DIR TO ROOT\n"
     ]
    }
   ],
   "source": [
    "# Necessary for importing modules from a sub-directory\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "if os.getcwd().split('/')[-1] == 'notebooks':\n",
    "    print(\"CHANGE DIR TO ROOT\")\n",
    "    os.chdir(r\"../\")\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "from evaluation import aggregate_metrics, get_metric_umass, get_metric_cv\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROD = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and load dataset\n",
    "\n",
    "if PROD:\n",
    "    DATASET_TYPE = 'BN'\n",
    "    DATASET_SIZE = '10000'\n",
    "    DATASET_INDEX = '1'\n",
    "    data_dir = f'var/data_prod_noun_propn'\n",
    "    #corpus = pickle.load(open(f'{data_dir}/efselab_{DATASET_TYPE}_{DATASET_SIZE}_{DATASET_INDEX}.pkl', 'rb'))\n",
    "    #NUM_ARTICLES = len(corpus)\n",
    "    ext_corpus = pickle.load(open(f'{data_dir}/efselab_extrinsic_20000.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50005"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1 = pickle.load(open(f'{data_dir}/efselab_{DATASET_TYPE}_{DATASET_SIZE}_1.pkl', 'rb'))\n",
    "c2 = pickle.load(open(f'{data_dir}/efselab_{DATASET_TYPE}_{DATASET_SIZE}_2.pkl', 'rb'))\n",
    "c3 = pickle.load(open(f'{data_dir}/efselab_{DATASET_TYPE}_{DATASET_SIZE}_3.pkl', 'rb'))\n",
    "c4 = pickle.load(open(f'{data_dir}/efselab_{DATASET_TYPE}_{DATASET_SIZE}_4.pkl', 'rb'))\n",
    "c5 = pickle.load(open(f'{data_dir}/efselab_{DATASET_TYPE}_{DATASET_SIZE}_5.pkl', 'rb'))\n",
    "corpus = c1 + c2 + c3 + c4 + c5\n",
    "NUM_ARTICLES = len(corpus)\n",
    "len(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL_NAME:  NMF_frobenius_mu_init-nndsvda_i-ALL\n",
      "METRIC_FILE_NAME:  metric_log/prod_noun_propn/BN10000/model=NMF_frobenius_mu_init-nndsvda_data=BN10000_ALL.csv\n",
      "\n",
      "RUNNING IN PRODUCTION MODE!\n"
     ]
    }
   ],
   "source": [
    "# Preprocess variables\n",
    "MIN_DF = 2   # float | int: 0.001-0.002 (~4000), 0.002 (~1000)\n",
    "MAX_DF = 0.95 # float (0.95)\n",
    "\n",
    "NGRAM = False # False | True\n",
    "if NGRAM is True:\n",
    "    NGRAM_OPT = 'ext+int' # 'ext+int' | 'int' \n",
    "else:\n",
    "    NGRAM_OPT = '-'\n",
    "\n",
    "# Run variables\n",
    "NUM_TOPICS = range(10,151, 10)\n",
    "\n",
    "# Model variables\n",
    "NMF_NORM = 'frobenius' # ‘frobenius’ | ‘kullback-leibler’ | ‘itakura-saito’\n",
    "NMF_SOLVER = 'mu' # ‘cd’ | ‘mu’\n",
    "INIT = 'nndsvda' # None | ‘random’ | ‘nndsvd’ | ‘nndsvda’ | ‘nndsvdar’ | ‘custom’\n",
    "ALPHA = 0    # float 0-1\n",
    "L1_RATIO = 0.0 # float 0-1\n",
    "\n",
    "MAX_ITERATIONS = 500\n",
    "MODEL_NAME = f'NMF_{NMF_NORM}_{NMF_SOLVER}'\n",
    "if INIT is not None:\n",
    "    MODEL_NAME = f'{MODEL_NAME}_init-{str(INIT)}'\n",
    "if (ALPHA != 0):\n",
    "    MODEL_NAME = f'{MODEL_NAME}_alpha-{ALPHA}_L1-{L1_RATIO}'\n",
    "\n",
    "if PROD:\n",
    "    METRIC_FILENAME = f'metric_log/prod_noun_propn/{DATASET_TYPE}{DATASET_SIZE}/model={MODEL_NAME}_data={DATASET_TYPE}{DATASET_SIZE}_ALL.csv'\n",
    "    MODEL_NAME = f'{MODEL_NAME}_i-ALL'\n",
    "else:\n",
    "    METRIC_FILENAME = f'metric_log/nmf_metrics_data={DATA_SET}{NUM_ARTICLES}_v10.csv'\n",
    "\n",
    "print('MODEL_NAME: ', MODEL_NAME)\n",
    "print('METRIC_FILE_NAME: ', METRIC_FILENAME)\n",
    "\n",
    "print()\n",
    "print('RUNNING IN PRODUCTION MODE!') if PROD else print('IN DEV MODE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define NGRAM, if applicable\n",
    "def get_ngram(corpus, level=3):\n",
    "    if len(corpus) < 1000:\n",
    "        bi_mc, bi_t = 10, 70\n",
    "        tri_mc, tri_t = 15, 50\n",
    "    elif (len(corpus) < 4000):\n",
    "        bi_mc, bi_t = 25, 250\n",
    "        tri_mc, tri_t = 25, 200\n",
    "    else:\n",
    "        bi_mc, bi_t = 45, 400\n",
    "        tri_mc, tri_t = 45, 400\n",
    "        \n",
    "    ngram = None\n",
    "    if level >= 2:\n",
    "        phrases_bigram = Phrases(corpus, min_count=bi_mc, threshold=bi_t, delimiter=b'_')\n",
    "        ngram = Phraser(phrases_bigram)\n",
    "    if level >= 3:\n",
    "        bigramed_corpus = [ngram[doc] for doc in corpus]\n",
    "        phrases_trigram = Phrases(bigramed_corpus, min_count=tri_mc, threshold=tri_t, delimiter=b'_')\n",
    "        ngram = Phraser(phrases_trigram)\n",
    "    return ngram\n",
    "\n",
    "#if (NGRAM is True):\n",
    "    #if NGRAM_OPT == 'ext+int':\n",
    "        #ngram = get_ngram(corpus+ext_corpus, level=3)\n",
    "    #else:\n",
    "        #ngram = get_ngram(corpus, level=3)\n",
    "    #corpus = [ngram[doc] for doc in corpus]\n",
    "    #ext_corpus = [ngram[doc] for doc in ext_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Features from tfidf is 130026\n"
     ]
    }
   ],
   "source": [
    "# Create TF-IDF\n",
    "vectorizer = TfidfVectorizer(\n",
    "    lowercase=False,\n",
    "    tokenizer=lambda x: x, \n",
    "    max_df=MAX_DF, \n",
    "    min_df=MIN_DF, max_features=None, use_idf=True)\n",
    "tf_idf = vectorizer.fit_transform(corpus)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "NUM_FEATURES = len(feature_names)\n",
    "print(f'#Features from tfidf is {NUM_FEATURES}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of topics (nested list)\n",
    "def get_NMF_topics(model, feature_names, n_top_words):\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topics.append([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "    return topics\n",
    "\n",
    "N_TOP_WORDS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62490 terms not in extrinsic corpus added to smooth_gensim_dict\n"
     ]
    }
   ],
   "source": [
    "# Format Gensim components for Coherence metrics\n",
    "ext_gensim_dict = corpora.Dictionary(ext_corpus)\n",
    "tok2id = ext_gensim_dict.token2id\n",
    "\n",
    "feature_terms_not_in_ext_corpus = [[term] for term in feature_names if term not in tok2id]\n",
    "smooth_gensim_dict = corpora.Dictionary(ext_corpus + feature_terms_not_in_ext_corpus)\n",
    "print(f'{len(feature_terms_not_in_ext_corpus)} terms not in extrinsic corpus added to smooth_gensim_dict')\n",
    "\n",
    "int_gensim_dict = corpora.Dictionary(corpus)\n",
    "int_gensim_corpus = [int_gensim_dict.doc2bow(text) for text in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 number of terms not in extrinsic corpus added as separate one-term documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  7%|▋         | 1/15 [00:38<09:05, 38.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 number of terms not in extrinsic corpus added as separate one-term documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 13%|█▎        | 2/15 [01:32<09:21, 43.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 number of terms not in extrinsic corpus added as separate one-term documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|██        | 3/15 [02:40<10:07, 50.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 number of terms not in extrinsic corpus added as separate one-term documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 27%|██▋       | 4/15 [03:57<10:44, 58.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 number of terms not in extrinsic corpus added as separate one-term documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 33%|███▎      | 5/15 [05:23<11:10, 67.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 number of terms not in extrinsic corpus added as separate one-term documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|████      | 6/15 [07:04<11:35, 77.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 number of terms not in extrinsic corpus added as separate one-term documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 47%|████▋     | 7/15 [09:03<11:56, 89.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 number of terms not in extrinsic corpus added as separate one-term documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 53%|█████▎    | 8/15 [11:29<12:26, 106.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 number of terms not in extrinsic corpus added as separate one-term documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|██████    | 9/15 [14:19<12:33, 125.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 number of terms not in extrinsic corpus added as separate one-term documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 67%|██████▋   | 10/15 [17:28<12:03, 144.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 number of terms not in extrinsic corpus added as separate one-term documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 73%|███████▎  | 11/15 [20:48<10:45, 161.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 number of terms not in extrinsic corpus added as separate one-term documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|████████  | 12/15 [24:23<08:52, 177.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 number of terms not in extrinsic corpus added as separate one-term documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 87%|████████▋ | 13/15 [28:10<06:24, 192.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 number of terms not in extrinsic corpus added as separate one-term documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 93%|█████████▎| 14/15 [31:29<03:14, 194.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 number of terms not in extrinsic corpus added as separate one-term documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [35:59<00:00, 143.94s/it]\n"
     ]
    }
   ],
   "source": [
    "for topic_index in tqdm(NUM_TOPICS):\n",
    "    # Fit model\n",
    "    t_start = time.time()\n",
    "\n",
    "    model = NMF(random_state=0, tol=0.0001, verbose=0, shuffle=False,\\\n",
    "                n_components=topic_index, beta_loss=NMF_NORM, solver=NMF_SOLVER, \\\n",
    "                init=INIT, alpha=ALPHA, l1_ratio=L1_RATIO, max_iter=MAX_ITERATIONS)\n",
    "\n",
    "    W = model.fit_transform(tf_idf, y=feature_names)\n",
    "    H = model.components_\n",
    "\n",
    "    RUN_TIME = round(time.time() - t_start, 3)\n",
    "    ITERATIONS = model.n_iter_\n",
    "    topics = get_NMF_topics(model, feature_names, N_TOP_WORDS)\n",
    "    \n",
    "    # Create smoothened corpus by extending ext corpus with 1-term documents containing missing topic terms\n",
    "    flat_topic_terms = [term for topic in topics for term in topic]\n",
    "    topic_terms_not_in_ext_corpus = [[term] for term in flat_topic_terms if term not in tok2id]\n",
    "    smooth_corpus = ext_corpus + topic_terms_not_in_ext_corpus\n",
    "    print(f'{len(topic_terms_not_in_ext_corpus)} number of terms not in extrinsic corpus added as separate one-term documents')\n",
    "\n",
    "\n",
    "    # Calculate Coherence metrics\n",
    "    m_cv = get_metric_cv(topics, smooth_corpus, smooth_gensim_dict)\n",
    "    coh_cv = [e[0] for e in m_cv]\n",
    "    std_cv = [e[1] for e in m_cv]\n",
    "    m_umass = get_metric_umass(topics, int_gensim_dict, int_gensim_corpus)\n",
    "    coh_umass = [e[0] for e in m_umass]\n",
    "    std_umass = [e[1] for e in m_umass]\n",
    "\n",
    "    reduced_cv = aggregate_metrics(coh_cv)\n",
    "    reduced_cv['std'] = round(np.mean(std_cv), 4)\n",
    "    reduced_umass = aggregate_metrics(coh_umass)\n",
    "    reduced_umass['std'] = round(np.mean(std_umass), 4)\n",
    "    \n",
    "    top_n_topics = 1\n",
    "    relative_sparseness = [sum(sorted(W[i], reverse=True)[:top_n_topics])/(sum(W[i]) if sum(W[i]) > 0 else 1) for i in range(len(W[:]))]\n",
    "    reduced_rs = aggregate_metrics(relative_sparseness)\n",
    "\n",
    "    # Save model to csv\n",
    "    DATE = datetime.now().strftime('%m%d-%H:%M')\n",
    "\n",
    "    #fields=[DATE, MODEL_NAME, NUM_ARTICLES, topic_index, ITERATIONS, RUN_TIME, NGRAM_OPT, \\\n",
    "    #        reduced_cv.get('avg'), reduced_cv.get('med'), reduced_cv.get('top'), reduced_cv.get('bot'), \\\n",
    "    #        reduced_umass.get('avg'), reduced_umass.get('med'), reduced_umass.get('top'), reduced_umass.get('bot'), \\\n",
    "    #        reduced_cv.get('std'), reduced_umass.get('std'), MIN_DF, NUM_FEATURES,'_']\n",
    "    \n",
    "    fields=[DATE, MODEL_NAME, NUM_ARTICLES, topic_index, ITERATIONS, RUN_TIME, NGRAM_OPT, NUM_FEATURES, \\\n",
    "        reduced_cv.get('avg'), reduced_cv.get('top'), reduced_cv.get('bot'), \\\n",
    "        reduced_umass.get('avg'), reduced_umass.get('top'), reduced_umass.get('bot'), \\\n",
    "        reduced_rs.get('avg'), reduced_rs.get('top'), reduced_rs.get('bot'), '_']\n",
    "    \n",
    "    if os.path.exists(METRIC_FILENAME):  \n",
    "        with open(METRIC_FILENAME, 'a+', newline='\\n', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f, delimiter=',')\n",
    "            writer.writerow(fields)\n",
    "        f.close()\n",
    "    else:\n",
    "        with open(METRIC_FILENAME, 'w', newline='\\n', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f, delimiter=',')\n",
    "            headers=['date', 'model', '#articles', '#topics', 'iterations', 'time', 'ngram', '#features', \\\n",
    "            'cv_avg', 'cv_top', 'cv_bot', \\\n",
    "            'umass_avg', 'umass_top', 'umass_bot', \\\n",
    "            'rs_avg', 'rs_top', 'rs_bot', \\\n",
    "             '_']\n",
    "            writer.writerow(headers)\n",
    "            writer.writerow(fields)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
